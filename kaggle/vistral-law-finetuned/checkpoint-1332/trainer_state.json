{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1332,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02252252252252252,
      "grad_norm": 9.952058792114258,
      "learning_rate": 1.999930466150119e-05,
      "loss": 2.6218,
      "step": 10
    },
    {
      "epoch": 0.04504504504504504,
      "grad_norm": 8.291156768798828,
      "learning_rate": 1.999374253369202e-05,
      "loss": 1.8985,
      "step": 20
    },
    {
      "epoch": 0.06756756756756757,
      "grad_norm": 17.381736755371094,
      "learning_rate": 1.998529007462963e-05,
      "loss": 2.1884,
      "step": 30
    },
    {
      "epoch": 0.09009009009009009,
      "grad_norm": 57.120235443115234,
      "learning_rate": 1.9971532122280466e-05,
      "loss": 1.8015,
      "step": 40
    },
    {
      "epoch": 0.11261261261261261,
      "grad_norm": 172.81881713867188,
      "learning_rate": 1.9953281320131466e-05,
      "loss": 1.7915,
      "step": 50
    },
    {
      "epoch": 0.13513513513513514,
      "grad_norm": 199.2621307373047,
      "learning_rate": 1.9927743329353295e-05,
      "loss": 1.9015,
      "step": 60
    },
    {
      "epoch": 0.15765765765765766,
      "grad_norm": 371.882080078125,
      "learning_rate": 1.9896683016861382e-05,
      "loss": 1.8798,
      "step": 70
    },
    {
      "epoch": 0.18018018018018017,
      "grad_norm": 8.255574226379395,
      "learning_rate": 1.986011765999988e-05,
      "loss": 1.4489,
      "step": 80
    },
    {
      "epoch": 0.20270270270270271,
      "grad_norm": 5.080233097076416,
      "learning_rate": 1.9818067598301894e-05,
      "loss": 1.4334,
      "step": 90
    },
    {
      "epoch": 0.22522522522522523,
      "grad_norm": 14.81053352355957,
      "learning_rate": 1.9770556222175607e-05,
      "loss": 1.5306,
      "step": 100
    },
    {
      "epoch": 0.24774774774774774,
      "grad_norm": 10.81022834777832,
      "learning_rate": 1.971760995989332e-05,
      "loss": 1.6796,
      "step": 110
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 5.183917999267578,
      "learning_rate": 1.9659258262890683e-05,
      "loss": 1.3612,
      "step": 120
    },
    {
      "epoch": 0.2927927927927928,
      "grad_norm": 39.1898193359375,
      "learning_rate": 1.959553358938431e-05,
      "loss": 1.4426,
      "step": 130
    },
    {
      "epoch": 0.3153153153153153,
      "grad_norm": 53.63801956176758,
      "learning_rate": 1.952647138631682e-05,
      "loss": 1.2499,
      "step": 140
    },
    {
      "epoch": 0.33783783783783783,
      "grad_norm": 5.012943267822266,
      "learning_rate": 1.945211006963945e-05,
      "loss": 1.3966,
      "step": 150
    },
    {
      "epoch": 0.36036036036036034,
      "grad_norm": 19.22540283203125,
      "learning_rate": 1.937249100294311e-05,
      "loss": 1.303,
      "step": 160
    },
    {
      "epoch": 0.38288288288288286,
      "grad_norm": 7.381397247314453,
      "learning_rate": 1.928765847444984e-05,
      "loss": 1.31,
      "step": 170
    },
    {
      "epoch": 0.40540540540540543,
      "grad_norm": 6.237392902374268,
      "learning_rate": 1.9197659672377388e-05,
      "loss": 1.1155,
      "step": 180
    },
    {
      "epoch": 0.42792792792792794,
      "grad_norm": 20.1495304107666,
      "learning_rate": 1.9102544658690746e-05,
      "loss": 1.3442,
      "step": 190
    },
    {
      "epoch": 0.45045045045045046,
      "grad_norm": 22.015579223632812,
      "learning_rate": 1.900236634125507e-05,
      "loss": 1.0648,
      "step": 200
    },
    {
      "epoch": 0.47297297297297297,
      "grad_norm": 9.804285049438477,
      "learning_rate": 1.8897180444405615e-05,
      "loss": 1.3057,
      "step": 210
    },
    {
      "epoch": 0.4954954954954955,
      "grad_norm": 10.967053413391113,
      "learning_rate": 1.8787045477950993e-05,
      "loss": 1.1539,
      "step": 220
    },
    {
      "epoch": 0.5180180180180181,
      "grad_norm": 31.686784744262695,
      "learning_rate": 1.8672022704627004e-05,
      "loss": 0.9196,
      "step": 230
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 19.3915958404541,
      "learning_rate": 1.8552176106019156e-05,
      "loss": 1.1876,
      "step": 240
    },
    {
      "epoch": 0.5630630630630631,
      "grad_norm": 14.030653953552246,
      "learning_rate": 1.8427572346972806e-05,
      "loss": 1.2012,
      "step": 250
    },
    {
      "epoch": 0.5855855855855856,
      "grad_norm": 25.661426544189453,
      "learning_rate": 1.8298280738510753e-05,
      "loss": 1.3315,
      "step": 260
    },
    {
      "epoch": 0.6081081081081081,
      "grad_norm": 249.09637451171875,
      "learning_rate": 1.8164373199278858e-05,
      "loss": 1.2054,
      "step": 270
    },
    {
      "epoch": 0.6306306306306306,
      "grad_norm": 5.629980087280273,
      "learning_rate": 1.802592421554123e-05,
      "loss": 1.2309,
      "step": 280
    },
    {
      "epoch": 0.6531531531531531,
      "grad_norm": 7.273730278015137,
      "learning_rate": 1.7883010799747098e-05,
      "loss": 1.0385,
      "step": 290
    },
    {
      "epoch": 0.6756756756756757,
      "grad_norm": 10.05869197845459,
      "learning_rate": 1.773571244769254e-05,
      "loss": 1.1907,
      "step": 300
    },
    {
      "epoch": 0.6981981981981982,
      "grad_norm": 11.231670379638672,
      "learning_rate": 1.7584111094300825e-05,
      "loss": 0.9752,
      "step": 310
    },
    {
      "epoch": 0.7207207207207207,
      "grad_norm": 9.033105850219727,
      "learning_rate": 1.7428291068045998e-05,
      "loss": 1.1745,
      "step": 320
    },
    {
      "epoch": 0.7432432432432432,
      "grad_norm": 6.878522872924805,
      "learning_rate": 1.7268339044045044e-05,
      "loss": 1.1437,
      "step": 330
    },
    {
      "epoch": 0.7657657657657657,
      "grad_norm": 11.7225341796875,
      "learning_rate": 1.7104343995844716e-05,
      "loss": 0.9898,
      "step": 340
    },
    {
      "epoch": 0.7882882882882883,
      "grad_norm": 32.98842239379883,
      "learning_rate": 1.693639714592988e-05,
      "loss": 1.1511,
      "step": 350
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 56.82328796386719,
      "learning_rate": 1.676459191498087e-05,
      "loss": 1.2992,
      "step": 360
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 11.449342727661133,
      "learning_rate": 1.6589023869908108e-05,
      "loss": 1.126,
      "step": 370
    },
    {
      "epoch": 0.8558558558558559,
      "grad_norm": 6.218727111816406,
      "learning_rate": 1.640979067069286e-05,
      "loss": 1.1104,
      "step": 380
    },
    {
      "epoch": 0.8783783783783784,
      "grad_norm": 9.96968936920166,
      "learning_rate": 1.6226992016063726e-05,
      "loss": 1.3023,
      "step": 390
    },
    {
      "epoch": 0.9009009009009009,
      "grad_norm": 12.910223007202148,
      "learning_rate": 1.604072958803909e-05,
      "loss": 1.0484,
      "step": 400
    },
    {
      "epoch": 0.9234234234234234,
      "grad_norm": 8.446237564086914,
      "learning_rate": 1.5851106995366337e-05,
      "loss": 1.0273,
      "step": 410
    },
    {
      "epoch": 0.9459459459459459,
      "grad_norm": 36.721004486083984,
      "learning_rate": 1.5658229715889345e-05,
      "loss": 0.959,
      "step": 420
    },
    {
      "epoch": 0.9684684684684685,
      "grad_norm": 5.100126266479492,
      "learning_rate": 1.5462205037876274e-05,
      "loss": 0.9752,
      "step": 430
    },
    {
      "epoch": 0.990990990990991,
      "grad_norm": 7.793125152587891,
      "learning_rate": 1.5263142000340312e-05,
      "loss": 0.9465,
      "step": 440
    },
    {
      "epoch": 1.0135135135135136,
      "grad_norm": 30.843963623046875,
      "learning_rate": 1.5061151332386565e-05,
      "loss": 0.8436,
      "step": 450
    },
    {
      "epoch": 1.0360360360360361,
      "grad_norm": 18.644250869750977,
      "learning_rate": 1.4856345391618829e-05,
      "loss": 0.9684,
      "step": 460
    },
    {
      "epoch": 1.0585585585585586,
      "grad_norm": 21.909912109375,
      "learning_rate": 1.464883810164052e-05,
      "loss": 0.9116,
      "step": 470
    },
    {
      "epoch": 1.0810810810810811,
      "grad_norm": 7.745554447174072,
      "learning_rate": 1.4438744888684481e-05,
      "loss": 0.8184,
      "step": 480
    },
    {
      "epoch": 1.1036036036036037,
      "grad_norm": 15.491930961608887,
      "learning_rate": 1.4226182617406996e-05,
      "loss": 0.9329,
      "step": 490
    },
    {
      "epoch": 1.1261261261261262,
      "grad_norm": 16.02051544189453,
      "learning_rate": 1.4011269525881636e-05,
      "loss": 0.9394,
      "step": 500
    },
    {
      "epoch": 1.1486486486486487,
      "grad_norm": 15.78524112701416,
      "learning_rate": 1.3794125159829173e-05,
      "loss": 1.1278,
      "step": 510
    },
    {
      "epoch": 1.1711711711711712,
      "grad_norm": 8.687480926513672,
      "learning_rate": 1.3574870306120078e-05,
      "loss": 0.7496,
      "step": 520
    },
    {
      "epoch": 1.1936936936936937,
      "grad_norm": 27.877872467041016,
      "learning_rate": 1.3353626925586674e-05,
      "loss": 0.8591,
      "step": 530
    },
    {
      "epoch": 1.2162162162162162,
      "grad_norm": 10.667618751525879,
      "learning_rate": 1.3130518085182224e-05,
      "loss": 0.8876,
      "step": 540
    },
    {
      "epoch": 1.2387387387387387,
      "grad_norm": 36.65496063232422,
      "learning_rate": 1.290566788952477e-05,
      "loss": 0.8789,
      "step": 550
    },
    {
      "epoch": 1.2612612612612613,
      "grad_norm": 7.087657928466797,
      "learning_rate": 1.2679201411863751e-05,
      "loss": 0.8881,
      "step": 560
    },
    {
      "epoch": 1.2837837837837838,
      "grad_norm": 23.979280471801758,
      "learning_rate": 1.2451244624507831e-05,
      "loss": 0.7119,
      "step": 570
    },
    {
      "epoch": 1.3063063063063063,
      "grad_norm": 5.843842029571533,
      "learning_rate": 1.2221924328752618e-05,
      "loss": 0.829,
      "step": 580
    },
    {
      "epoch": 1.3288288288288288,
      "grad_norm": 11.535987854003906,
      "learning_rate": 1.1991368084347252e-05,
      "loss": 0.7446,
      "step": 590
    },
    {
      "epoch": 1.3513513513513513,
      "grad_norm": 16.636171340942383,
      "learning_rate": 1.1759704138539121e-05,
      "loss": 0.886,
      "step": 600
    },
    {
      "epoch": 1.3738738738738738,
      "grad_norm": 92.44588470458984,
      "learning_rate": 1.152706135473613e-05,
      "loss": 0.7738,
      "step": 610
    },
    {
      "epoch": 1.3963963963963963,
      "grad_norm": 20.815086364746094,
      "learning_rate": 1.129356914082624e-05,
      "loss": 0.7449,
      "step": 620
    },
    {
      "epoch": 1.4189189189189189,
      "grad_norm": 9.47230052947998,
      "learning_rate": 1.1059357377194161e-05,
      "loss": 0.6614,
      "step": 630
    },
    {
      "epoch": 1.4414414414414414,
      "grad_norm": 21.96399688720703,
      "learning_rate": 1.0824556344475182e-05,
      "loss": 0.8435,
      "step": 640
    },
    {
      "epoch": 1.4639639639639639,
      "grad_norm": 16.289134979248047,
      "learning_rate": 1.0589296651086376e-05,
      "loss": 0.9645,
      "step": 650
    },
    {
      "epoch": 1.4864864864864864,
      "grad_norm": 52.93164825439453,
      "learning_rate": 1.0353709160575488e-05,
      "loss": 0.9496,
      "step": 660
    },
    {
      "epoch": 1.509009009009009,
      "grad_norm": 12.650433540344238,
      "learning_rate": 1.0117924918827891e-05,
      "loss": 0.8359,
      "step": 670
    },
    {
      "epoch": 1.5315315315315314,
      "grad_norm": 12.539258003234863,
      "learning_rate": 9.882075081172114e-06,
      "loss": 1.0534,
      "step": 680
    },
    {
      "epoch": 1.554054054054054,
      "grad_norm": 15.817412376403809,
      "learning_rate": 9.646290839424515e-06,
      "loss": 0.8834,
      "step": 690
    },
    {
      "epoch": 1.5765765765765765,
      "grad_norm": 9.324373245239258,
      "learning_rate": 9.410703348913626e-06,
      "loss": 0.9611,
      "step": 700
    },
    {
      "epoch": 1.599099099099099,
      "grad_norm": 16.620420455932617,
      "learning_rate": 9.175443655524822e-06,
      "loss": 0.7601,
      "step": 710
    },
    {
      "epoch": 1.6216216216216215,
      "grad_norm": 14.563952445983887,
      "learning_rate": 8.94064262280584e-06,
      "loss": 0.7484,
      "step": 720
    },
    {
      "epoch": 1.644144144144144,
      "grad_norm": 22.67400550842285,
      "learning_rate": 8.706430859173764e-06,
      "loss": 0.7481,
      "step": 730
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 13.54118824005127,
      "learning_rate": 8.472938645263875e-06,
      "loss": 0.8135,
      "step": 740
    },
    {
      "epoch": 1.689189189189189,
      "grad_norm": 12.649402618408203,
      "learning_rate": 8.24029586146088e-06,
      "loss": 0.9361,
      "step": 750
    },
    {
      "epoch": 1.7117117117117115,
      "grad_norm": 9.461493492126465,
      "learning_rate": 8.00863191565275e-06,
      "loss": 0.8612,
      "step": 760
    },
    {
      "epoch": 1.7342342342342343,
      "grad_norm": 11.469368934631348,
      "learning_rate": 7.778075671247386e-06,
      "loss": 0.8189,
      "step": 770
    },
    {
      "epoch": 1.7567567567567568,
      "grad_norm": 40.31538391113281,
      "learning_rate": 7.548755375492173e-06,
      "loss": 0.8117,
      "step": 780
    },
    {
      "epoch": 1.7792792792792793,
      "grad_norm": 11.483484268188477,
      "learning_rate": 7.320798588136253e-06,
      "loss": 0.7174,
      "step": 790
    },
    {
      "epoch": 1.8018018018018018,
      "grad_norm": 44.72285079956055,
      "learning_rate": 7.094332110475235e-06,
      "loss": 0.8494,
      "step": 800
    },
    {
      "epoch": 1.8243243243243243,
      "grad_norm": 20.34132194519043,
      "learning_rate": 6.869481914817779e-06,
      "loss": 0.6938,
      "step": 810
    },
    {
      "epoch": 1.8468468468468469,
      "grad_norm": 40.86142349243164,
      "learning_rate": 6.64637307441333e-06,
      "loss": 0.8982,
      "step": 820
    },
    {
      "epoch": 1.8693693693693694,
      "grad_norm": 7.069985866546631,
      "learning_rate": 6.4251296938799255e-06,
      "loss": 0.7212,
      "step": 830
    },
    {
      "epoch": 1.8918918918918919,
      "grad_norm": 20.659774780273438,
      "learning_rate": 6.205874840170833e-06,
      "loss": 0.6937,
      "step": 840
    },
    {
      "epoch": 1.9144144144144144,
      "grad_norm": 39.38510513305664,
      "learning_rate": 5.988730474118367e-06,
      "loss": 0.7706,
      "step": 850
    },
    {
      "epoch": 1.936936936936937,
      "grad_norm": 13.819793701171875,
      "learning_rate": 5.773817382593008e-06,
      "loss": 1.0089,
      "step": 860
    },
    {
      "epoch": 1.9594594594594594,
      "grad_norm": 8.305798530578613,
      "learning_rate": 5.561255111315525e-06,
      "loss": 0.8528,
      "step": 870
    },
    {
      "epoch": 1.981981981981982,
      "grad_norm": 24.092418670654297,
      "learning_rate": 5.351161898359485e-06,
      "loss": 0.7372,
      "step": 880
    },
    {
      "epoch": 2.0045045045045047,
      "grad_norm": 31.170949935913086,
      "learning_rate": 5.1436546083811725e-06,
      "loss": 0.7876,
      "step": 890
    },
    {
      "epoch": 2.027027027027027,
      "grad_norm": 13.934741020202637,
      "learning_rate": 4.938848667613436e-06,
      "loss": 0.7126,
      "step": 900
    },
    {
      "epoch": 2.0495495495495497,
      "grad_norm": 23.360397338867188,
      "learning_rate": 4.7368579996596906e-06,
      "loss": 0.6571,
      "step": 910
    },
    {
      "epoch": 2.0720720720720722,
      "grad_norm": 30.983205795288086,
      "learning_rate": 4.537794962123726e-06,
      "loss": 0.7243,
      "step": 920
    },
    {
      "epoch": 2.0945945945945947,
      "grad_norm": 19.845199584960938,
      "learning_rate": 4.341770284110655e-06,
      "loss": 0.652,
      "step": 930
    },
    {
      "epoch": 2.1171171171171173,
      "grad_norm": 40.28609085083008,
      "learning_rate": 4.148893004633663e-06,
      "loss": 0.6959,
      "step": 940
    },
    {
      "epoch": 2.1396396396396398,
      "grad_norm": 12.270796775817871,
      "learning_rate": 3.959270411960913e-06,
      "loss": 0.8255,
      "step": 950
    },
    {
      "epoch": 2.1621621621621623,
      "grad_norm": 11.382634162902832,
      "learning_rate": 3.7730079839362755e-06,
      "loss": 0.7542,
      "step": 960
    },
    {
      "epoch": 2.184684684684685,
      "grad_norm": 13.820918083190918,
      "learning_rate": 3.590209329307143e-06,
      "loss": 0.7626,
      "step": 970
    },
    {
      "epoch": 2.2072072072072073,
      "grad_norm": 10.853487968444824,
      "learning_rate": 3.410976130091892e-06,
      "loss": 0.6656,
      "step": 980
    },
    {
      "epoch": 2.22972972972973,
      "grad_norm": 11.658759117126465,
      "learning_rate": 3.2354080850191328e-06,
      "loss": 0.6673,
      "step": 990
    },
    {
      "epoch": 2.2522522522522523,
      "grad_norm": 14.727378845214844,
      "learning_rate": 3.0636028540701235e-06,
      "loss": 0.6635,
      "step": 1000
    },
    {
      "epoch": 2.274774774774775,
      "grad_norm": 8.93548583984375,
      "learning_rate": 2.8956560041552884e-06,
      "loss": 0.6917,
      "step": 1010
    },
    {
      "epoch": 2.2972972972972974,
      "grad_norm": 61.59888458251953,
      "learning_rate": 2.7316609559549568e-06,
      "loss": 0.7779,
      "step": 1020
    },
    {
      "epoch": 2.31981981981982,
      "grad_norm": 56.196868896484375,
      "learning_rate": 2.5717089319540003e-06,
      "loss": 0.7314,
      "step": 1030
    },
    {
      "epoch": 2.3423423423423424,
      "grad_norm": 26.722614288330078,
      "learning_rate": 2.4158889056991773e-06,
      "loss": 0.7787,
      "step": 1040
    },
    {
      "epoch": 2.364864864864865,
      "grad_norm": 10.293006896972656,
      "learning_rate": 2.2642875523074613e-06,
      "loss": 0.6207,
      "step": 1050
    },
    {
      "epoch": 2.3873873873873874,
      "grad_norm": 27.2296142578125,
      "learning_rate": 2.1169892002529046e-06,
      "loss": 0.5937,
      "step": 1060
    },
    {
      "epoch": 2.40990990990991,
      "grad_norm": 10.337726593017578,
      "learning_rate": 1.974075784458771e-06,
      "loss": 0.653,
      "step": 1070
    },
    {
      "epoch": 2.4324324324324325,
      "grad_norm": 12.262860298156738,
      "learning_rate": 1.8356268007211442e-06,
      "loss": 0.5268,
      "step": 1080
    },
    {
      "epoch": 2.454954954954955,
      "grad_norm": 43.42727279663086,
      "learning_rate": 1.7017192614892507e-06,
      "loss": 0.6189,
      "step": 1090
    },
    {
      "epoch": 2.4774774774774775,
      "grad_norm": 12.645801544189453,
      "learning_rate": 1.5724276530271965e-06,
      "loss": 0.7564,
      "step": 1100
    },
    {
      "epoch": 2.5,
      "grad_norm": 60.52812576293945,
      "learning_rate": 1.4478238939808454e-06,
      "loss": 0.8509,
      "step": 1110
    },
    {
      "epoch": 2.5225225225225225,
      "grad_norm": 9.413897514343262,
      "learning_rate": 1.3279772953729985e-06,
      "loss": 0.571,
      "step": 1120
    },
    {
      "epoch": 2.545045045045045,
      "grad_norm": 32.849273681640625,
      "learning_rate": 1.2129545220490102e-06,
      "loss": 0.7501,
      "step": 1130
    },
    {
      "epoch": 2.5675675675675675,
      "grad_norm": 31.0440731048584,
      "learning_rate": 1.1028195555943877e-06,
      "loss": 0.5789,
      "step": 1140
    },
    {
      "epoch": 2.59009009009009,
      "grad_norm": 14.899283409118652,
      "learning_rate": 9.976336587449309e-07,
      "loss": 0.7595,
      "step": 1150
    },
    {
      "epoch": 2.6126126126126126,
      "grad_norm": 60.97958755493164,
      "learning_rate": 8.974553413092557e-07,
      "loss": 0.6737,
      "step": 1160
    },
    {
      "epoch": 2.635135135135135,
      "grad_norm": 11.45573616027832,
      "learning_rate": 8.023403276226127e-07,
      "loss": 0.5818,
      "step": 1170
    },
    {
      "epoch": 2.6576576576576576,
      "grad_norm": 14.950140953063965,
      "learning_rate": 7.123415255501654e-07,
      "loss": 0.9592,
      "step": 1180
    },
    {
      "epoch": 2.68018018018018,
      "grad_norm": 11.980090141296387,
      "learning_rate": 6.275089970568882e-07,
      "loss": 0.6462,
      "step": 1190
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 10.4385404586792,
      "learning_rate": 5.478899303605512e-07,
      "loss": 0.7178,
      "step": 1200
    },
    {
      "epoch": 2.725225225225225,
      "grad_norm": 26.282602310180664,
      "learning_rate": 4.735286136831807e-07,
      "loss": 0.6979,
      "step": 1210
    },
    {
      "epoch": 2.7477477477477477,
      "grad_norm": 9.68556022644043,
      "learning_rate": 4.044664106156915e-07,
      "loss": 0.5114,
      "step": 1220
    },
    {
      "epoch": 2.77027027027027,
      "grad_norm": 12.862146377563477,
      "learning_rate": 3.4074173710931804e-07,
      "loss": 0.6172,
      "step": 1230
    },
    {
      "epoch": 2.7927927927927927,
      "grad_norm": 39.37451934814453,
      "learning_rate": 2.823900401066837e-07,
      "loss": 0.739,
      "step": 1240
    },
    {
      "epoch": 2.815315315315315,
      "grad_norm": 13.754348754882812,
      "learning_rate": 2.2944377782439632e-07,
      "loss": 0.693,
      "step": 1250
    },
    {
      "epoch": 2.8378378378378377,
      "grad_norm": 12.539485931396484,
      "learning_rate": 1.8193240169810943e-07,
      "loss": 0.6755,
      "step": 1260
    },
    {
      "epoch": 2.8603603603603602,
      "grad_norm": 17.986663818359375,
      "learning_rate": 1.398823400001237e-07,
      "loss": 0.6303,
      "step": 1270
    },
    {
      "epoch": 2.8828828828828827,
      "grad_norm": 13.491604804992676,
      "learning_rate": 1.0331698313861938e-07,
      "loss": 0.5838,
      "step": 1280
    },
    {
      "epoch": 2.9054054054054053,
      "grad_norm": 10.776825904846191,
      "learning_rate": 7.225667064670761e-08,
      "loss": 0.6907,
      "step": 1290
    },
    {
      "epoch": 2.9279279279279278,
      "grad_norm": 47.85564041137695,
      "learning_rate": 4.671867986853374e-08,
      "loss": 0.6349,
      "step": 1300
    },
    {
      "epoch": 2.9504504504504503,
      "grad_norm": 13.535538673400879,
      "learning_rate": 2.671721634873725e-08,
      "loss": 0.7886,
      "step": 1310
    },
    {
      "epoch": 2.972972972972973,
      "grad_norm": 9.519639015197754,
      "learning_rate": 1.2263405930585947e-08,
      "loss": 0.6354,
      "step": 1320
    },
    {
      "epoch": 2.9954954954954953,
      "grad_norm": 7.80157470703125,
      "learning_rate": 3.365288567216407e-09,
      "loss": 0.5391,
      "step": 1330
    }
  ],
  "logging_steps": 10,
  "max_steps": 1332,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1276314631995392e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
